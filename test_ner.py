import os
import torch
from transformers import BertTokenizerFast, BertForTokenClassification

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ëª¨ë¸ ë¡œë“œ
BASE_DIR = os.path.dirname(__file__)
MODEL_PATH = os.path.join(BASE_DIR, "ner_model")

tokenizer_loc = BertTokenizerFast.from_pretrained(MODEL_PATH)
model_loc = BertForTokenClassification.from_pretrained(MODEL_PATH)
model_loc.config.id2label = {0: "O", 1: "B-ORG", 2: "B-LOC", 3: "I-LOC", 4: "I-ORG"}
model_loc.config.label2id = {v: k for k, v in model_loc.config.id2label.items()}
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_loc.to(device)
model_loc.eval()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_locations(text: str) -> list:
    """
    B-LOC/I-LOCë¡œ ì˜ˆì¸¡ëœ ì„œë¸Œì›Œë“œë¥¼ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ê²°í•©í•´, ì—°ì†ëœ ì§€ëª… ë‹¨ì–´ë¥¼ ìŠ¤íŒ¬ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not text:
        return []
    # í† í°í™”: word level ì¸ë±ìŠ¤ í¬í•¨
    encoding = tokenizer_loc(
        text,
        return_offsets_mapping=True,
        return_tensors="pt",
        truncation=True,
        max_length=512,
        is_split_into_words=True
    )
    input_ids = encoding["input_ids"].to(device)
    attention_mask = encoding["attention_mask"].to(device)
    word_ids = encoding.word_ids()
    tokens = tokenizer_loc.convert_ids_to_tokens(input_ids[0])

    # ëª¨ë¸ ì˜ˆì¸¡
    with torch.no_grad():
        logits = model_loc(input_ids=input_ids, attention_mask=attention_mask).logits[0]
    preds = torch.argmax(logits, dim=-1).tolist()

    # word_id ë‹¨ìœ„ë¡œ subword ê²°í•© ë° word-level LOC íŒì •
    word_texts = []
    word_is_loc = []
    current_id = None
    pieces = []
    labels = []
    for tok, pid, wid in zip(tokens, preds, word_ids):
        if wid is None:
            continue
        label = model_loc.config.id2label[pid]
        if wid != current_id:
            if current_id is not None:
                # finalize previous word
                word = ''.join([p[2:] if p.startswith('##') else p for p in pieces])
                is_loc = any(l in ('B-LOC','I-LOC') for l in labels)
                word_texts.append(word)
                word_is_loc.append(is_loc)
            # reset for new word
            current_id = wid
            pieces = [tok]
            labels = [label]
        else:
            pieces.append(tok)
            labels.append(label)
    # ë§ˆì§€ë§‰ ë‹¨ì–´ ì²˜ë¦¬
    if current_id is not None:
        word = ''.join([p[2:] if p.startswith('##') else p for p in pieces])
        is_loc = any(l in ('B-LOC','I-LOC') for l in labels)
        word_texts.append(word)
        word_is_loc.append(is_loc)

    # ì—°ì† LOC ë‹¨ì–´ë¥¼ ë¬¶ì–´ ìŠ¤íŒ¬ ìƒì„±
    spans = []
    current_span = []
    for word, is_loc in zip(word_texts, word_is_loc):
        if is_loc:
            current_span.append(word)
        else:
            if current_span:
                spans.append(' '.join(current_span))
                current_span = []
    if current_span:
        spans.append(' '.join(current_span))

    return spans


def extract_location_tokens(text: str) -> list:
    """
    B-LOC/I-LOC ì„œë¸Œì›Œë“œ í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not text:
        return []
    enc = tokenizer_loc(text, return_tensors="pt", truncation=True, max_length=512)
    input_ids = enc["input_ids"][0]
    tokens = tokenizer_loc.convert_ids_to_tokens(input_ids)
    enc = {k: v.to(device) for k, v in enc.items()}
    with torch.no_grad():
        logits = model_loc(**enc).logits[0]
    preds = torch.argmax(logits, dim=-1).tolist()
    return [tok for tok, pid in zip(tokens, preds) if model_loc.config.id2label[pid] in ('B-LOC','I-LOC')]


def debug_token_labels(text: str) -> list:
    """
    ëª¨ë“  í† í°ê³¼ ì˜ˆì¸¡ ë¼ë²¨ì„ íŠœí”Œë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    enc = tokenizer_loc(text, return_tensors="pt", truncation=True, max_length=512)
    input_ids = enc["input_ids"][0]
    tokens = tokenizer_loc.convert_ids_to_tokens(input_ids)
    enc = {k: v.to(device) for k, v in enc.items()}
    with torch.no_grad():
        logits = model_loc(**enc).logits[0]
    preds = torch.argmax(logits, dim=-1).tolist()
    return [(tok, model_loc.config.id2label[pid]) for tok, pid in zip(tokens, preds)]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    examples = [
        "21:48 ë‚¨êµ¬ ì„ ì•”ë™(ë‘ì™•ì‚¬ê±°ë¦¬, ê°ë‚˜ë¬´ì§„ì‚¬ê±°ë¦¬ ë°©ë©´)í™”ì¬ ë°œìƒ. ì¸ê·¼ ì£¼ë¯¼ì€ ì£¼ì˜ ë°”ëë‹ˆë‹¤.",
        "ì˜¤ëŠ˜ 13:48 ì˜¨ì–‘ì ìš´í™”ë¦¬ ì‚°119-1 ì‚°ë¶ˆ ë°œìƒ. ë§ˆì„ ì£¼ë¯¼ì€ ëŒ€í”¼í•˜ì„¸ìš”.",
        "ì„œìš¸íŠ¹ë³„ì‹œ ì„±ë™êµ¬ ì™•ì‹­ë¦¬ë¡œ 123, í•œì–‘ëŒ€ ì• íš¡ë‹¨ë³´ë„ ì¸ê·¼ì—ì„œ ì‚¬ê³  ë°œìƒ.",
        "ê¸°ìƒì²­ ì˜ˆë³´: ê²½ê¸°ë„ ìˆ˜ì›ì‹œ ê¶Œì„ êµ¬ í˜¸ë§¤ì‹¤ë™ì— í˜¸ìš°ì£¼ì˜ë³´ ë°œíš¨ ì¤‘."
    ]
    for sent in examples:
        print(f"ë¬¸ì¥: {sent}")
        print(f"ì „ì²˜ë¦¬ëœ ë‹¨ì–´: {tokenizer_loc.convert_ids_to_tokens(tokenizer_loc(sent, return_tensors='pt', truncation=True, max_length=512)['input_ids'][0])}")
        print(f"í† í°-ë¼ë²¨: {debug_token_labels(sent)}")
        print(f"ì¶”ì¶œëœ ìŠ¤íŒ¬: {extract_locations(sent)}")
        print(f"ì¶”ì¶œëœ í† í°: {extract_location_tokens(sent)}")
        print("â”€" * 40)
    print("ğŸ” ì§ì ‘ ì…ë ¥ í…ŒìŠ¤íŠ¸ (ì¢…ë£Œí•˜ë ¤ë©´ ë¹ˆ ì¤„ ì…ë ¥)")
    while True:
        text = input("ë¬¸ì¥ ì…ë ¥> ").strip()
        if not text:
            break
        print(f"ë‹¨ì–´ ë‹¨ìœ„: {tokenizer_loc.convert_ids_to_tokens(tokenizer_loc(text, return_tensors='pt', truncation=True, max_length=512)['input_ids'][0])}")
        print(f"í† í°-ë¼ë²¨: {debug_token_labels(text)}")
        print(f"ìŠ¤íŒ¬: {extract_locations(text)}")
        print(f"í† í°: {extract_location_tokens(text)}\n")
